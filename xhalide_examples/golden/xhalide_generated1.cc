// [AI-CodeGen]:  Code generated by XHalide compiler for AI architecture with vector intrinsics.
#include<stdint.h>
#define XHALIDE_SRS_SHIFT 10


int conv(int16_t  * restrict _b0_buffer, int _b0_buffer_dim0_min, int _b0_buffer_dim0_extent, int _b0_buffer_dim0_stride, int _b0_buffer_dim1_min, int _b0_buffer_dim1_extent, int _b0_buffer_dim1_stride, 
int16_t  * restrict _b1_buffer, int _b1_buffer_dim0_min, int _b1_buffer_dim0_extent, int _b1_buffer_dim0_stride, int _b1_buffer_dim1_min, int _b1_buffer_dim1_extent, int _b1_buffer_dim1_stride, 
int16_t  * restrict _conv_buffer, int _conv_buffer_dim0_min, int _conv_buffer_dim0_extent, int _conv_buffer_dim0_stride, int _conv_buffer_dim1_min, int _conv_buffer_dim1_extent, int _conv_buffer_dim1_stride) {
 void *_0 = _b0_buffer;
 void * _b0 = _0;
 int32_t _1 = _b0_buffer_dim0_min;
 int32_t _2 = _b0_buffer_dim0_extent;
 int32_t _3 = _b0_buffer_dim0_stride;
 int32_t _4 = _b0_buffer_dim1_min;
 int32_t _5 = _b0_buffer_dim1_extent;
 int32_t _6 = _b0_buffer_dim1_stride;
 void *_7 = _b1_buffer;
 void * _b1 = _7;
 int32_t _8 = _b1_buffer_dim0_min;
 int32_t _9 = _b1_buffer_dim0_extent;
 int32_t _10 = _b1_buffer_dim0_stride;
 int32_t _11 = _b1_buffer_dim1_min;
 int32_t _12 = _b1_buffer_dim1_extent;
 int32_t _13 = _b1_buffer_dim1_stride;
 void *_14 = _conv_buffer;
 void * _conv = _14;
 int32_t _15 = _conv_buffer_dim0_min;
 int32_t _16 = _conv_buffer_dim0_stride;
 int32_t _17 = _conv_buffer_dim1_min;
 int32_t _18 = _conv_buffer_dim1_stride;
 // produce conv
 int32_t _19 = _17 * _18;
 int32_t _20 = _19 + _15;
 for (int _conv_s1_y_y = 0; _conv_s1_y_y < 0 + 4; _conv_s1_y_y++)
 {
  int32_t _21 = _conv_s1_y_y * 5;
  int32_t _22 = _conv_s1_y_y * _18;
  int32_t _23 = _22 - _20;
  for (int _conv_s1_x_x_x = 0; _conv_s1_x_x_x < 0 + 4; _conv_s1_x_x_x++)
  chess_prepare_for_pipelining chess_loop_range(4, 4)
  {
   v16acc48 out_0_acc;
   v32int16 loads_temporal_group_0 = undef_v32int16(); 
   int32_t _24 = _21 + _conv_s1_x_x_x;
   int32_t _25 = _24 * 16;
   loads_temporal_group_0 = upd_w(loads_temporal_group_0, 0, *(v16int16* ) (_b0_buffer+_25+0)); 
   v16int16 loads_spatial_group_0 = *(v16int16* ) (_b1_buffer+0); 
   loads_temporal_group_0 = upd_w(loads_temporal_group_0, 1, *(v16int16* ) (_b0_buffer+_25+16)); 
   out_0_acc = mul16(loads_temporal_group_0, 0, 0x03020100, 0x07060504, 0x2110, loads_spatial_group_0, 0, 0, 0, 1);
   out_0_acc = mac16(out_0_acc,loads_temporal_group_0, 2, 0x03020100, 0x07060504, 0x2110, loads_spatial_group_0, 2, 0, 0, 1);
   v32int16 loads_temporal_group_1 = undef_v32int16(); 
   int32_t _26 = _25 + 80;
   loads_temporal_group_1 = upd_w(loads_temporal_group_1, 0, *(v16int16* ) (_b0_buffer+_26+0)); 
   loads_temporal_group_1 = upd_w(loads_temporal_group_1, 1, *(v16int16* ) (_b0_buffer+_26+16)); 
   out_0_acc = mac16(out_0_acc,loads_temporal_group_1, 0, 0x03020100, 0x07060504, 0x2110, loads_spatial_group_0, 4, 0, 0, 1);
   out_0_acc = mac16(out_0_acc,loads_temporal_group_1, 2, 0x03020100, 0x07060504, 0x2110, loads_spatial_group_0, 6, 0, 0, 1);
   v32int16 loads_temporal_group_2 = undef_v32int16(); 
   int32_t _27 = _25 + 160;
   loads_temporal_group_2 = upd_w(loads_temporal_group_2, 0, *(v16int16* ) (_b0_buffer+_27+0)); 
   loads_temporal_group_2 = upd_w(loads_temporal_group_2, 1, *(v16int16* ) (_b0_buffer+_27+16)); 
   out_0_acc = mac16(out_0_acc,loads_temporal_group_2, 0, 0x03020100, 0x07060504, 0x2110, loads_spatial_group_0, 8, 0, 0, 1);
   out_0_acc = mac16(out_0_acc,loads_temporal_group_2, 2, 0x03020100, 0x07060504, 0x2110, loads_spatial_group_0, 10, 0, 0, 1);
   int32_t _28 = _conv_s1_x_x_x * 16;
   int32_t _29 = _28 + _23;
   *(v16int16*)(_conv_buffer+_29) = srs(out_0_acc, XHALIDE_SRS_SHIFT);
  } // for _conv_s1_x_x_x
 } // for _conv_s1_y_y
 return 0;
}
